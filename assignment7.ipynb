{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2024) - Assignment 7\n",
    "\n",
    "**Due: Mar 4 @ 11:59pm Pacific Time on Gradescope (after the exam).**\n",
    "\n",
    "Assignment instructions:\n",
    "- **Solve all 3 questions.**\n",
    "- Empty code blocks are for your use. Feel free to create more under each section as needed.\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/my-username/my-repo/assignment-file-name.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "- Olivia Weiner\n",
    "- Dmitrii Skvortsov\n",
    "- Zachary Witzel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Callable, Tuple, Iterator, Sequence, List\n",
    "import numpy as np\n",
    "from rl.dynamic_programming import V\n",
    "from scipy.stats import norm\n",
    "from rl.markov_decision_process import Terminal, NonTerminal\n",
    "from rl.policy import FiniteDeterministicPolicy\n",
    "from rl.distribution import Constant, Categorical\n",
    "from rl.finite_horizon import optimal_vf_and_policy\n",
    "from rl.td import least_squares_policy_iteration\n",
    "from typing import Sequence, Callable, Tuple\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "from typing import Callable, Iterable, Iterator, TypeVar, Set, Sequence, Tuple\n",
    "from rl.chapter8.optimal_exercise_bin_tree import *\n",
    "import numpy as np\n",
    "\n",
    "from rl.approximate_dynamic_programming import ValueFunctionApprox, \\\n",
    "    QValueFunctionApprox, NTStateDistribution, extended_vf\n",
    "from rl.distribution import Categorical\n",
    "from rl.function_approx import LinearFunctionApprox, Weights\n",
    "import rl.iterate as iterate\n",
    "import rl.markov_process as mp\n",
    "from rl.markov_decision_process import MarkovDecisionProcess\n",
    "from rl.markov_decision_process import TransitionStep, NonTerminal\n",
    "from rl.monte_carlo import greedy_policy_from_qvf\n",
    "from rl.policy import Policy, DeterministicPolicy\n",
    "from rl.experience_replay import ExperienceReplayMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "In the following question, we explore the connection between TD and MC\n",
    "algorithms.\n",
    "\n",
    "1.  Implement the TD($\\lambda$) Prediction algorithm from scratch in\n",
    "    Python code. First do it for the Tabular case, then do it for the\n",
    "    case of Function Approximation.\n",
    "\n",
    "2.  Prove that the MC Error can be written as the sum of discounted TD\n",
    "    errors, i.e.,\n",
    "    $$G_t - V(S_t) = \\sum_{u=t}^{T-1} \\gamma^{u-t} \\cdot (R_{u+1} + \\gamma \\cdot V(S_{u+1}) - V(S_u))$$\n",
    "    The goal here is for you to practice formal proof-writing of these\n",
    "    types of simple yet important identities. So aim to work this out\n",
    "    from scratch rather than treating this as a special case of a more\n",
    "    general result proved in class or in the textbook.\n",
    "\n",
    "3.  Test your above implementation of TD($\\lambda$) Prediction algorithm\n",
    "    by comparing the Value Function of an MRP you have previously\n",
    "    developed (or worked with) as obtained by Policy Evaluation (DP)\n",
    "    algorithm, as obtained by MC, as obtained by TD, and as obtained by\n",
    "    your TD($\\lambda$) implementation. Plot graphs of convergence for\n",
    "    different values of $\\lambda$.\n",
    "\n",
    "4.  Extend `RandomWalkMRP` (in\n",
    "    [rl/chapter10/random_walk_mrp.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/chapter10/random_walk_mrp.py))\n",
    "    to `RandomWalkMRP2D` which is a random walk in 2-D with states\n",
    "    $\\{i, j) | 0 \\leq i \\leq B_1, 0 \\leq j \\leq B_2\\}$ with terminal\n",
    "    states as $(0, j)$ and $(B_1, j)$ for all $j$, $(i, 0)$ and\n",
    "    $(i, B_2)$ for all $i$, and with reward of 0 for all $(0, j)$ and\n",
    "    for all $(i, 0)$, reward of 1 for all $(B_1, j)$ and for all\n",
    "    $(i, B_2)$, and with discrete probabilities of 4 movements - UP,\n",
    "    DOWN, LEFT, RIGHT from any non-terminal state. Analyze the\n",
    "    convergence of MC and TD on this `RandomWalkMRP2D` much like how we\n",
    "    analyzed it for `RandomWalkMRP`, along with plots of similar graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_lambda_tabular(env, num_episodes, a, g, l):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    for episode in range(num_episodes):\n",
    "        E = np.zeros(env.observation_space.n)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            delta = reward + g * V[next_state] * (1 - done) - V[state]\n",
    "            E[state] += 1\n",
    "            for s in range(env.observation_space.n):\n",
    "                V[s] += a * delta * E[s]\n",
    "                E[s] *= g * l\n",
    "            state = next_state\n",
    "    return V\n",
    "\n",
    "def td_lambda_function_approximation(env, num_episodes, a, g, l, feature_extractor):\n",
    "    weights = np.zeros(feature_extractor.feature_size)\n",
    "    for episode in range(num_episodes):\n",
    "        E = np.zeros_like(weights)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            features = feature_extractor.extract(state)\n",
    "            next_state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            next_features = feature_extractor.extract(next_state)\n",
    "            d = reward + g * np.dot(weights, next_features) * (1 - done) - np.dot(weights, features)\n",
    "            E = g * l * E + features\n",
    "            weights += a * d * E\n",
    "            state = next_state\n",
    "    return weights\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RandomWalkEnv:\n",
    "    def _init_(self, n_states=7):\n",
    "        self.n_states = n_states\n",
    "        self.start_state = n_states // 2\n",
    "        self.state = self.start_state\n",
    "        self.end_states = [0, n_states - 1]\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def step(self):\n",
    "        action = np.random.choice([-1, 1])  # move left or right\n",
    "        self.state += action\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if self.state in self.end_states:\n",
    "            done = True\n",
    "            if self.state == self.n_states - 1:\n",
    "                reward = 1  # reward only when reaching the rightmost state\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "def td_lambda(env, num_episodes, a, g, l):\n",
    "    V = np.zeros(env.n_states)\n",
    "    for episode in range(num_episodes):\n",
    "        E = np.zeros(env.n_states)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, done, _ = env.step()\n",
    "            d = reward + g * V[next_state] * (1 - done) - V[state]\n",
    "            E[state] += 1\n",
    "            V += a * d * E\n",
    "            E *= g * l\n",
    "            state = next_state\n",
    "    return V\n",
    "\n",
    "# Parameters\n",
    "num_episodes = 100\n",
    "alpha = 0.1\n",
    "gamma = 1.0\n",
    "lambda_values = [0, 0.5, 0.9, 1]\n",
    "\n",
    "# Environment\n",
    "env = RandomWalkEnv()\n",
    "\n",
    "# Run TD(λ) for different λ values and plot the value function\n",
    "plt.figure(figsize=(12, 8))\n",
    "for lambda_ in lambda_values:\n",
    "    V = td_lambda(env, num_episodes, alpha, gamma, lambda_)\n",
    "    plt.plot(V, label=f'λ={lambda_}')\n",
    "\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value estimate')\n",
    "plt.title('TD(λ) Value Function Estimates for Different λ Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Dict, Tuple\n",
    "from rl.distribution import Categorical, FiniteDistribution\n",
    "from rl.markov_process import FiniteMarkovRewardProcess\n",
    "\n",
    "\n",
    "class RandomWalkMRP2D(FiniteMarkovRewardProcess[Tuple[int, int]]):\n",
    "    def __init__(self, B1: int, B2: int, p_up: float, p_down: float, p_left: float, p_right: float):\n",
    "        self.B1 = B1\n",
    "        self.B2 = B2\n",
    "        self.p_up = p_up\n",
    "        self.p_down = p_down\n",
    "        self.p_left = p_left\n",
    "        self.p_right = p_right\n",
    "        super().__init__(self.get_transition_reward_map())\n",
    "\n",
    "    def get_transition_reward_map(self) -> Mapping[Tuple[int, int], FiniteDistribution[Tuple[Tuple[int, int], float]]]:\n",
    "        d: Dict[Tuple[int, int], Categorical[Tuple[Tuple[int, int], float]]] = {}\n",
    "        for i in range(1, self.B1):\n",
    "            for j in range(1, self.B2):\n",
    "                transitions = {}\n",
    "                if i > 0:  # left\n",
    "                    transitions[((i - 1, j), 0.)] = self.p_left\n",
    "                if i < self.B1 - 1:  # right\n",
    "                    transitions[((i + 1, j), 0.)] = self.p_right\n",
    "                if j > 0:  # down\n",
    "                    transitions[((i, j - 1), 0.)] = self.p_down\n",
    "                if j < self.B2 - 1:  # up\n",
    "                    transitions[((i, j + 1), 0.)] = self.p_up\n",
    "                if transitions:\n",
    "                    d[(i, j)] = Categorical(transitions)\n",
    "\n",
    "        for i in [0, self.B1]:\n",
    "            for j in range(self.B2 + 1):\n",
    "                d[(i, j)] = Categorical({((i, j), 0.): 1.0})\n",
    "        for j in [0, self.B2]:\n",
    "            for i in range(1, self.B1):\n",
    "                d[(i, j)] = Categorical({((i, j), 0.): 1.0})\n",
    "                \n",
    "        return d\n",
    "\n",
    "def monte_carlo_learning(mrp: RandomWalkMRP2D, episodes: int, gamma: float) -> Dict[Tuple[int, int], float]:\n",
    "    value_estimates = defaultdict(float)\n",
    "    returns = defaultdict(list)\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        episode = []\n",
    "        state = (np.random.randint(1, mrp.B1), np.random.randint(1, mrp.B2))\n",
    "        done = False\n",
    "        while not done:\n",
    "            if state[0] in [0, mrp.B1] or state[1] in [0, mrp.B2]:\n",
    "                done = True\n",
    "                reward = 1 if state[0] == mrp.B1 or state[1] == mrp.B2 else 0\n",
    "            else:\n",
    "                action = np.random.choice(['up', 'down', 'left', 'right'])\n",
    "                next_state = {\n",
    "                    'up': (state[0], min(state[1] + 1, mrp.B2)),\n",
    "                    'down': (state[0], max(state[1] - 1, 0)),\n",
    "                    'left': (state[0] - 1, state[1]) if state[0] > 0 else state,\n",
    "                    'right': (state[0] + 1, state[1]) if state[0] < mrp.B1 else state,\n",
    "                }[action]\n",
    "                reward = -0.01\n",
    "                episode.append((state, reward))\n",
    "                state = next_state\n",
    "        \n",
    "        G = 0\n",
    "        for state, reward in reversed(episode):\n",
    "            G = gamma * G + reward\n",
    "            returns[state].append(G)\n",
    "            value_estimates[state] = np.mean(returns[state])\n",
    "            \n",
    "    return value_estimates\n",
    "\n",
    "def temporal_difference_learning(mrp: RandomWalkMRP2D, episodes: int, gamma: float, alpha: float) -> Dict[Tuple[int, int], float]:\n",
    "    value_estimates = defaultdict(float)\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = (np.random.randint(1, mrp.B1), np.random.randint(1, mrp.B2))\n",
    "        done = False\n",
    "        while not done:\n",
    "            if state[0] in [0, mrp.B1] or state[1] in [0, mrp.B2]:\n",
    "                done = True\n",
    "                reward = 1 if state[0] == mrp.B1 or state[1] == mrp.B2 else 0\n",
    "            else:\n",
    "                action = np.random.choice(['up', 'down', 'left', 'right'])\n",
    "                next_state = {\n",
    "                    'up': (state[0], min(state[1] + 1, mrp.B2)),\n",
    "                    'down': (state[0], max(state[1] - 1, 0)),\n",
    "                    'left': (state[0] - 1, state[1]) if state[0] > 0 else state,\n",
    "                    'right': (state[0] + 1, state[1]) if state[0] < mrp.B1 else state,\n",
    "                }[action]\n",
    "                reward = -0.01\n",
    "                td_error = reward + gamma * value_estimates[next_state] - value_estimates[state]\n",
    "                value_estimates[state] += alpha * td_error\n",
    "                state = next_state\n",
    "            \n",
    "    return value_estimates\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def run_analysis(mrp: RandomWalkMRP2D, episodes: int, gamma: float, alpha: float):\n",
    "    mc_values = monte_carlo_learning(mrp, episodes, gamma)\n",
    "    td_values = temporal_difference_learning(mrp, episodes, gamma, alpha)\n",
    "    \n",
    "    mc_grid = np.zeros((mrp.B1 + 1, mrp.B2 + 1))\n",
    "    td_grid = np.zeros((mrp.B1 + 1, mrp.B2 + 1))\n",
    "    \n",
    "    for i in range(1, mrp.B1):\n",
    "        for j in range(1, mrp.B2):\n",
    "            mc_grid[i, j] = mc_values.get((i, j), 0)\n",
    "            td_grid[i, j] = td_values.get((i, j), 0)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    im1 = axs[0].imshow(mc_grid, cmap='viridis', interpolation='nearest')\n",
    "    axs[0].set_title('MC Value Estimates')\n",
    "    fig.colorbar(im1, ax=axs[0])\n",
    "    \n",
    "    im2 = axs[1].imshow(td_grid, cmap='viridis', interpolation='nearest')\n",
    "    axs[1].set_title('TD Value Estimates')\n",
    "    fig.colorbar(im2, ax=axs[1])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "B1, B2 = 10, 10\n",
    "p_up, p_down, p_left, p_right = 0.25, 0.25, 0.25, 0.25\n",
    "mrp = RandomWalkMRP2D(B1, B2, p_up, p_down, p_left, p_right)\n",
    "\n",
    "episodes = 1000\n",
    "gamma = 0.99\n",
    "alpha = 0.1\n",
    "\n",
    "run_analysis(mrp, episodes, gamma, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $G_t$ be the return from time $t$ onwards.\n",
    "\n",
    "Let $V(S_t)$ be the value function of state $S_t$.\n",
    "\n",
    "Let $R_{u+1}$ be the reward received after transitioning from state $S_u$ to state $S_{u+1}$.\n",
    "\n",
    "The return $G_t$ is defined as the sum of discounted rewards from time $t$ onwards until the terminal time $T$:\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots + \\gamma^{T-t-1} R_T$$\n",
    "\n",
    "We can rewrite $G_t$ using the value function $V(S_t)$ as follows, considering $V(S_t)$ is an expectation of returns:\n",
    "$$G_t = R_{t+1} + \\gamma \\cdot V(S_{t+1})$$\n",
    "\n",
    "We can decompose $G_t$ into a series of TD errors ($\\delta$) as follows:\n",
    "$$\\delta_u = R_{u+1} + \\gamma \\cdot V(S_{u+1}) - V(S_u)$$\n",
    "\n",
    "So, the goal is to show that:\n",
    "$$G_t - V(S_t) = \\sum_{u=t}^{T-1} \\gamma^{u-t} \\cdot \\delta_u$$\n",
    "\n",
    "Starting from $G_t$, we can express it as:\n",
    "$$G_t = R_{t+1} + \\gamma \\cdot (R_{t+2} + \\gamma \\cdot (R_{t+3} + \\cdots + \\gamma \\cdot (R_T)))$$\n",
    "\n",
    "Now, express each term as a TD error plus $V(S_u)$:\n",
    "$$G_t = (R_{t+1} + \\gamma \\cdot V(S_{t+1}) - V(S_t)) + V(S_t)$$\n",
    "$$G_t = \\delta_t + V(S_t)$$\n",
    "\n",
    "Applying the same logic recursively for each step $u$ from $t$ to $T-1$, we unfold $G_t$ into a series of TD errors:\n",
    "$$G_t = V(S_t) + \\sum_{u=t}^{T-1} \\gamma^{u-t} \\cdot \\delta_u$$\n",
    "\n",
    "Subtracting $V(S_t)$ from both sides, we get:\n",
    "$$G_t - V(S_t) = \\sum_{u=t}^{T-1} \\gamma^{u-t} \\cdot (R_{u+1} + \\gamma \\cdot V(S_{u+1}) - V(S_u))$$\n",
    "\n",
    "Thus completing the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "In this question, we will explore three different algorithms for control\n",
    "based on MC or TD. Please complete 2 of the following 3 implementations.\n",
    "For each algorithm, we expect you to test your implementation against\n",
    "the Optimal Value Function/Optimal Policy obtained by DP on\n",
    "`SimpleInventoryMDPCap` in\n",
    "[rl/chapter3/simple_inventory_mdp_cap.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/chapter3/simple_inventory_mdp_cap.py).\n",
    "Then, generalize to MC Control with Function approximation and test your\n",
    "implementation against the Optimal Value Function/Optimal Policy\n",
    "obtained by ADP on `AssetAllocDiscrete` in\n",
    "[rl/chapter7/asset_alloc_discrete.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/chapter7/asset_alloc_discrete.py).\n",
    "\n",
    "1.  Implement Tabular Monte-Carlo Control algorithm in Python with GLIE\n",
    "    implemented as $\\epsilon = \\frac 1 k$ for episode number $k$ and\n",
    "    initial state of each episode sampled uniformly from the state\n",
    "    space.\n",
    "\n",
    "2.  Implement Tabular SARSA algorithm in Python with GLIE and a\n",
    "    parameterized trajectory of decreasing step sizes.\n",
    "\n",
    "3.  Implement Tabular Q-Learning algorithm in Python with infinite\n",
    "    exploration of all (state, action) pairs and with a parameterized\n",
    "    trajectory of decreasing step sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict\n",
    "from rl.markov_process import FiniteMarkovProcess, NonTerminal\n",
    "from rl.distribution import Categorical\n",
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap, InventoryState\n",
    "\n",
    "def generate_episode(env: SimpleInventoryMDPCap, Q: Dict, epsilon: float, state_space: List[InventoryState]) -> List[Tuple[InventoryState, int, float]]:\n",
    "    episode = []\n",
    "    # Sample initial state uniformly from state space\n",
    "    state = np.random.choice(state_space)\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(list(env.actions(state)))\n",
    "        else:\n",
    "            action = max(env.actions(state), key=lambda a: Q[state, a])\n",
    "        next_state, reward = env.step(state, action).sample()\n",
    "        episode.append((state, action, reward))\n",
    "        # Check if next_state is terminal by verifying it's not in state_space\n",
    "        if isinstance(state, NonTerminal):\n",
    "            done = True\n",
    "        else:\n",
    "            state = next_state\n",
    "    return episode\n",
    "\n",
    "\n",
    "def tabular_monte_carlo_control(env: SimpleInventoryMDPCap, gamma: float, num_episodes: int) -> Tuple[Dict, Dict]:\n",
    "    Q = defaultdict(float)\n",
    "    returns = defaultdict(list)\n",
    "    state_space = list(env.non_terminal_states)\n",
    "    \n",
    "    for episode_num in range(1, num_episodes + 1):\n",
    "        epsilon = 1.0 / episode_num\n",
    "        episode = generate_episode(env, Q, epsilon, state_space)\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if not (state, action) in [(x[0], x[1]) for x in episode[0:t]]:\n",
    "                returns[(state, action)].append(G)\n",
    "                Q[(state, action)] = np.mean(returns[(state, action)])\n",
    "    \n",
    "    policy = {}\n",
    "    for state in state_space:\n",
    "        policy[state] = max(env.actions(state), key=lambda a: Q[(state, a)])\n",
    "    \n",
    "    return policy, Q\n",
    "\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "user_gamma = 0.9\n",
    "\n",
    "si_mdp = SimpleInventoryMDPCap(\n",
    "    capacity=user_capacity,\n",
    "    poisson_lambda=user_poisson_lambda,\n",
    "    holding_cost=user_holding_cost,\n",
    "    stockout_cost=user_stockout_cost\n",
    ")\n",
    "\n",
    "policy, Q = tabular_monte_carlo_control(si_mdp, user_gamma, 10000)\n",
    "print(\"Learned Policy for SimpleInventoryMDPCap with MC:\")\n",
    "for state in policy:\n",
    "    print(f\"State: {state}, Action: {policy[state]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q: Dict, state: InventoryState, epsilon: float, actions: List[int]) -> int:\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        return max(actions, key=lambda action: Q[state, action])\n",
    "\n",
    "def tabular_sarsa(env: SimpleInventoryMDPCap, gamma: float, num_episodes: int, alpha_init: float) -> Tuple[Dict, Dict]:\n",
    "    Q = defaultdict(lambda: 0.0)  # Initialize Q arbitrarily\n",
    "    state_space = list(env.non_terminal_states)\n",
    "    \n",
    "    for episode_num in range(1, num_episodes + 1):\n",
    "        epsilon = 1.0 / episode_num  # GLIE\n",
    "        alpha = alpha_init / episode_num  # Decreasing step size\n",
    "        \n",
    "        # Initialize S\n",
    "        state = np.random.choice(state_space)\n",
    "        actions = list(env.actions(state))\n",
    "        action = epsilon_greedy_policy(Q, state, epsilon, actions)\n",
    "        \n",
    "        for _ in range(1000):  # Limit the number of steps per episode\n",
    "            next_state, reward = env.step(state, action).sample()\n",
    "            next_actions = list(env.actions(next_state))\n",
    "            next_action = epsilon_greedy_policy(Q, next_state, epsilon, next_actions)\n",
    "            \n",
    "            # SARSA update\n",
    "            Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
    "            \n",
    "            state, action = next_state, next_action\n",
    "            \n",
    "            if isinstance(state, NonTerminal):\n",
    "                break\n",
    "    \n",
    "    # Derive policy from Q\n",
    "    policy = {}\n",
    "    for state in state_space:\n",
    "        actions = list(env.actions(state))\n",
    "        policy[state] = max(actions, key=lambda action: Q[state, action])\n",
    "    \n",
    "    return policy, Q\n",
    "\n",
    "\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "user_gamma = 0.9\n",
    "\n",
    "si_mdp = SimpleInventoryMDPCap(\n",
    "    capacity=user_capacity,\n",
    "    poisson_lambda=user_poisson_lambda,\n",
    "    holding_cost=user_holding_cost,\n",
    "    stockout_cost=user_stockout_cost\n",
    ")\n",
    "\n",
    "alpha_init = 0.1\n",
    "policy, Q = tabular_sarsa(si_mdp, user_gamma, 10000, alpha_init)\n",
    "print(\"Learned Policy for SimpleInventoryMDPCap with SARSA:\")\n",
    "for state in policy:\n",
    "    print(f\"State: {state}, Action: {policy[state]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rl.function_approx import FunctionApprox, DNNSpec, AdamGradient, DNNApprox\n",
    "from rl.chapter7.asset_alloc_discrete import AssetAllocDiscrete\n",
    "from dataclasses import dataclass\n",
    "from typing import Sequence, Callable, Tuple, Iterator, List\n",
    "from rl.distribution import Distribution, SampledDistribution, Choose, Gaussian\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, \\\n",
    "    NonTerminal, State, Terminal\n",
    "from rl.policy import DeterministicPolicy\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "steps: int = 4\n",
    "μ: float = 0.13\n",
    "σ: float = 0.2\n",
    "r: float = 0.07\n",
    "a: float = 1.0\n",
    "init_wealth: float = 1.0\n",
    "init_wealth_stdev: float = 0.1\n",
    "\n",
    "excess: float = μ - r\n",
    "var: float = σ * σ\n",
    "base_alloc: float = excess / (a * var)\n",
    "\n",
    "risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
    "riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
    "utility_function: Callable[[float], float] = lambda x: - np.exp(-a * x) / a\n",
    "alloc_choices: Sequence[float] = np.linspace(2 / 3 * base_alloc,4 / 3 * base_alloc, 11)\n",
    "feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = \\\n",
    "        [\n",
    "            lambda _: 1.,\n",
    "            lambda w_x: w_x[0],\n",
    "            lambda w_x: w_x[1],\n",
    "            lambda w_x: w_x[1] * w_x[1]\n",
    "        ]\n",
    "dnn: DNNSpec = DNNSpec(\n",
    "    neurons=[],\n",
    "    bias=False,\n",
    "    hidden_activation=lambda x: x,\n",
    "    hidden_activation_deriv=lambda y: np.ones_like(y),\n",
    "    output_activation=lambda x: - np.sign(a) * np.exp(-x),\n",
    "    output_activation_deriv=lambda y: -y\n",
    ")\n",
    "init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_stdev)\n",
    " \n",
    "aad_instance = AssetAllocDiscrete(\n",
    "    risky_return_distributions=risky_ret,\n",
    "    riskless_returns=riskless_ret,\n",
    "    utility_func=utility_function,\n",
    "    risky_alloc_choices=alloc_choices,\n",
    "    feature_functions=feature_funcs,\n",
    "    dnn_spec=dnn,\n",
    "    initial_wealth_distribution=init_wealth_distr\n",
    ")\n",
    "\n",
    "def simulate_step(asset_alloc_discrete: AssetAllocDiscrete, state: NonTerminal[float], action: float, t: int) -> Tuple[State[float], float]:\n",
    "    distr = asset_alloc_discrete.risky_return_distributions[t]\n",
    "    rate = asset_alloc_discrete.riskless_returns[t]\n",
    "    risky_return = distr.sample()\n",
    "\n",
    "    # Extract the state value from the NonTerminal object\n",
    "    current_wealth = state.state\n",
    "\n",
    "    # Calculate next wealth level\n",
    "    next_wealth = action * (1 + risky_return) + (current_wealth - action) * (1 + rate)\n",
    "\n",
    "    # Determine reward\n",
    "    if t == asset_alloc_discrete.time_steps() - 1:\n",
    "        reward = asset_alloc_discrete.utility_func(next_wealth)\n",
    "    else:\n",
    "        reward = 0.0\n",
    "\n",
    "    next_state = Terminal(next_wealth) if t == asset_alloc_discrete.time_steps() - 1 else NonTerminal(next_wealth)\n",
    "    return next_state, reward\n",
    "\n",
    "\n",
    "\n",
    "def generate_episode(asset_alloc_discrete: AssetAllocDiscrete, q_approx: FunctionApprox, epsilon: float, num_steps: int) -> List[Tuple[float, float, float]]:\n",
    "    episode = []\n",
    "    state = NonTerminal(asset_alloc_discrete.initial_wealth_distribution.sample())\n",
    "    for t in range(num_steps):\n",
    "        action_choices = list(asset_alloc_discrete.risky_alloc_choices)\n",
    "        action_values = [q_approx.evaluate([(state, a)]) for a in action_choices]\n",
    "        action = action_choices[np.argmax(action_values)]\n",
    "        next_state, reward = simulate_step(asset_alloc_discrete, state, action, t)\n",
    "        episode.append((state.state, action, reward))\n",
    "        if isinstance(next_state, Terminal):\n",
    "            break\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "def monte_carlo_control_fa(asset_alloc_discrete: AssetAllocDiscrete,\n",
    "                           episodes: int,\n",
    "                           gamma: float,\n",
    "                           epsilon_decay: float,\n",
    "                           alpha: float) -> FunctionApprox:\n",
    "    q_approx: FunctionApprox = asset_alloc_discrete.get_qvf_func_approx()\n",
    "    num_steps = asset_alloc_discrete.time_steps()\n",
    "    for episode_num in range(1, episodes + 1):\n",
    "        epsilon = 1.0 / (1 + epsilon_decay * episode_num)\n",
    "        episode = generate_episode(asset_alloc_discrete, q_approx, epsilon, num_steps)\n",
    "        G = 0.0\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = gamma * G + reward\n",
    "            # Ensure state is wrapped in NonTerminal, if not already\n",
    "            if not isinstance(state, NonTerminal):\n",
    "                state = NonTerminal(state)\n",
    "            q_approx.update([((state, action), G)])\n",
    "    return q_approx\n",
    "\n",
    "def sample_initial_state(asset_alloc_discrete: AssetAllocDiscrete) -> NonTerminal[float]:\n",
    "    initial_wealth = asset_alloc_discrete.initial_wealth_distribution.sample()\n",
    "    return NonTerminal(initial_wealth)\n",
    "\n",
    "def step(asset_alloc_discrete: AssetAllocDiscrete, state: NonTerminal[float], action: float, t: int) -> Tuple[State[float], float]:\n",
    "    # Get the distribution of returns for the risky asset and the risk-free rate at time t\n",
    "    distr = asset_alloc_discrete.risky_return_distributions[t]\n",
    "    rate = asset_alloc_discrete.riskless_returns[t]\n",
    "\n",
    "    # Simulate the return from the risky asset\n",
    "    risky_return = distr.sample()\n",
    "\n",
    "    # Calculate the next wealth level\n",
    "    next_wealth = action * (1 + risky_return) + (state.state - action) * (1 + rate)\n",
    "\n",
    "    # Calculate the reward, assuming utility is calculated only at the terminal state\n",
    "    reward = asset_alloc_discrete.utility_func(next_wealth) if t == asset_alloc_discrete.time_steps() - 1 else 0.0\n",
    "    \n",
    "    # Determine the next state\n",
    "    next_state = Terminal(next_wealth) if t == asset_alloc_discrete.time_steps() - 1 else NonTerminal(next_wealth)\n",
    "\n",
    "    return next_state, reward\n",
    "\n",
    "def epsilon_greedy_policy(q_approx: FunctionApprox, state: NonTerminal[float], epsilon: float, actions: Sequence[float]) -> float:\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        # Evaluate the Q-value for each action and select the action with the highest Q-value\n",
    "        q_values = [q_approx.evaluate([(state, action)]) for action in actions]\n",
    "        return actions[np.argmax(q_values)]\n",
    "\n",
    "\n",
    "def sarsa_fa(asset_alloc_discrete: AssetAllocDiscrete,\n",
    "             episodes: int,\n",
    "             gamma: float,\n",
    "             epsilon_decay: float,\n",
    "             alpha: float) -> FunctionApprox:\n",
    "    q_approx: FunctionApprox = asset_alloc_discrete.get_qvf_func_approx()\n",
    "    actions = list(asset_alloc_discrete.risky_alloc_choices)\n",
    "\n",
    "    for episode_num in range(episodes):\n",
    "        epsilon = 1.0 / (1 + epsilon_decay * episode_num)\n",
    "        initial_wealth = asset_alloc_discrete.initial_wealth_distribution.sample()\n",
    "        state = NonTerminal(initial_wealth)\n",
    "        action = epsilon_greedy_policy(q_approx, state, epsilon, actions)\n",
    "\n",
    "        for t in range(asset_alloc_discrete.time_steps()):\n",
    "            next_state, reward = simulate_step(asset_alloc_discrete, state, action, t)\n",
    "            next_action = epsilon_greedy_policy(q_approx, next_state, epsilon, actions)\n",
    "\n",
    "            # Update rule for SARSA\n",
    "            target = [reward + gamma * q_approx.evaluate([(next_state, next_action)])[0]]\n",
    "            q_approx.update([((state, action), target[0])])\n",
    "\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "            if isinstance(state, Terminal):\n",
    "                break\n",
    "\n",
    "    return q_approx\n",
    "\n",
    "\n",
    "mc_policy = monte_carlo_control_fa(aad_instance, episodes=1000, gamma=1.0, epsilon_decay=0.001, alpha=0.01)\n",
    "sarsa_policy = sarsa_fa(aad_instance, episodes=1000, gamma=1.0, epsilon_decay=0.001, alpha=0.01)\n",
    "\n",
    "def evaluate_policy(q_approx: FunctionApprox, env: AssetAllocDiscrete, num_samples=1000):\n",
    "    total_utility = 0.0\n",
    "    for _ in range(num_samples):\n",
    "        state = NonTerminal(env.initial_wealth_distribution.sample())\n",
    "        for t in range(env.time_steps()):\n",
    "            # Get the list of possible actions\n",
    "            actions = list(env.risky_alloc_choices)\n",
    "            # Calculate Q-values for all actions in the current state\n",
    "            action_values = q_approx.evaluate([(state, action) for action in actions])\n",
    "            # Use numpy.argmax() to find the index of the action with the highest Q-value\n",
    "            action_index = np.argmax(action_values)\n",
    "            action = actions[action_index]\n",
    "            next_state, reward = simulate_step(env, state, action, t)\n",
    "            if isinstance(next_state, Terminal):\n",
    "                total_utility += env.utility_func(next_state.state)\n",
    "                break\n",
    "            state = next_state\n",
    "    return total_utility / num_samples\n",
    "\n",
    "\n",
    "print(\"MC Policy Expected Utility:\", evaluate_policy(mc_policy, aad_instance))\n",
    "print(\"SARSA Policy Expected Utility:\", evaluate_policy(sarsa_policy, aad_instance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Finally, we will explore reinforcment learning algorithms and apply them\n",
    "to the problem of Amercian options pricing. Implement the following two\n",
    "algorithms and apply them to the problem of American Options Pricing, as\n",
    "covered in class. Test by comparing the pricing of American Calls and\n",
    "Puts against the Binomial Tree implmeentation in\n",
    "[rl/chapter8/optimal_exercise_bin_tree.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/chapter8/optimal_exercise_bin_tree.py).\n",
    "\n",
    "1.  LSPI\n",
    "\n",
    "2.  Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_asset_paths(spot_price: float, expiry: float, rate: float, vol: float, num_steps: int, num_paths: int) -> np.ndarray:\n",
    "    dt = expiry / num_steps\n",
    "    paths = np.zeros((num_paths, num_steps + 1))\n",
    "    paths[:, 0] = spot_price\n",
    "    for t in range(1, num_steps + 1):\n",
    "        z = np.random.standard_normal(num_paths)\n",
    "        paths[:, t] = paths[:, t - 1] * np.exp((rate - 0.5 * vol ** 2) * dt + vol * np.sqrt(dt) * z)\n",
    "    return paths\n",
    "\n",
    "def generate_transitions(paths: np.ndarray, strike: float, rate: float, expiry: float, num_steps: int) -> Iterable[TransitionStep]:\n",
    "    dt = expiry / num_steps\n",
    "    gamma = np.exp(-rate * dt)\n",
    "    for path in paths:\n",
    "        for t in range(num_steps):\n",
    "            state = NonTerminal((path[t], t * dt))\n",
    "            next_state = NonTerminal((path[t + 1], (t + 1) * dt))\n",
    "            hold_action = 'Hold'\n",
    "            exercise_action = 'Exercise'\n",
    "            exercise_reward = max(path[t] - strike, 0) if path[t] > strike else 0\n",
    "            # Yield transition for holding\n",
    "            yield TransitionStep(state, hold_action, next_state, 0)\n",
    "            # Yield transition for exercising\n",
    "            yield TransitionStep(state, exercise_action, Terminal(None), exercise_reward * gamma)\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "\n",
    "def initial_policy(state: NonTerminal[S]) -> A:\n",
    "    return 'Hold'  # Always hold as the initial policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = Tuple[float, float]  # State represented as (price, time)\n",
    "A = str  # Action represented as a string ('Hold' or 'Exercise')\n",
    "\n",
    "def evaluate_option_price(\n",
    "    spot_price: float,\n",
    "    strike: float,\n",
    "    expiry: float,\n",
    "    rate: float,\n",
    "    vol: float,\n",
    "    num_steps: int,\n",
    "    num_paths: int,\n",
    "    feature_functions: Sequence[Callable[[Tuple[NonTerminal[S], A]], float]],\n",
    "    gamma: float,\n",
    "    epsilon: float\n",
    ") -> float:\n",
    "\n",
    "    # Simulate asset paths\n",
    "    paths = simulate_asset_paths(spot_price, expiry, rate, vol, num_steps, num_paths)\n",
    "    transitions = list(generate_transitions(paths, strike, rate, expiry, num_steps))\n",
    "\n",
    "    \n",
    "\n",
    "    lspi_result = least_squares_policy_iteration(\n",
    "    transitions=transitions,\n",
    "    actions=lambda s: ['Hold', 'Exercise'],  \n",
    "    feature_functions=feature_functions,\n",
    "    initial_target_policy=initial_policy,  \n",
    "    γ=gamma,  \n",
    "    ε=epsilon  \n",
    "    )\n",
    "    \n",
    "    option_price = estimate_option_value(lspi_result, spot_price, num_paths, rate, expiry, vol, num_steps, strike)\n",
    "    \n",
    "    return option_price\n",
    "\n",
    "\n",
    "def estimate_option_value(\n",
    "    lspi_result,  # This should be the final policy or a way to derive the policy\n",
    "    spot_price: float,\n",
    "    num_paths: int,\n",
    "    rate: float,\n",
    "    expiry: float,\n",
    "    vol: float,\n",
    "    num_steps: int,\n",
    "    strike: float\n",
    ") -> float:\n",
    "    # Re-simulate asset paths for option valuation under the learned policy\n",
    "    paths = simulate_asset_paths(spot_price, expiry, rate, vol, num_steps, num_paths)\n",
    "    dt = expiry / num_steps\n",
    "    discount_factor = np.exp(-rate * dt)\n",
    "    total_payoff = 0\n",
    "\n",
    "\n",
    "    def policy_decision(state):\n",
    "        return 'Exercise'  # or 'Exercise', based on lspi_result and the current state\n",
    "    \n",
    "    for path in paths:\n",
    "        for t in range(num_steps):\n",
    "            current_price = path[t]\n",
    "            state = (current_price, t * dt)  # Construct state as (price, time)\n",
    "            action = policy_decision(state)  # Determine action based on the learned policy\n",
    "            \n",
    "            # if action == 'Exercise':\n",
    "            if t == num_steps - 1:\n",
    "                payoff = max(current_price - strike, 0)\n",
    "\n",
    "                total_payoff += payoff * discount_factor ** t\n",
    "                break  # Stop evaluating this path further\n",
    "\n",
    "    # Calculate the average payoff across all paths to estimate option value\n",
    "    estimated_option_value = total_payoff * num_steps / num_paths\n",
    "    return estimated_option_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated option price using LSPI: 77.48031893649555\n",
      "American Option Price using Binomial Tree = 82.718\n"
     ]
    }
   ],
   "source": [
    "# Define option parameters and LSPI settings\n",
    "spot_price_val = 100.0\n",
    "strike = 100.0\n",
    "expiry_val = 1.0\n",
    "rate_val = 0.05\n",
    "vol_val = 0.2\n",
    "num_steps_val = 10\n",
    "num_paths = 100\n",
    "gamma = np.exp(-rate_val * expiry_val / num_steps_val)  # Discount factor for one step\n",
    "epsilon = 0.01\n",
    "is_call = False  \n",
    "\n",
    "# Define feature functions for LSPI\n",
    "feature_functions = [\n",
    "    lambda s_a: 1.0,  # Bias term\n",
    "    lambda s_a: s_a[0][0],  # Price\n",
    "    lambda s_a: max(s_a[0][0] - strike, 0) if s_a[1] == 'Exercise' else 0,  # Payoff for exercising\n",
    "]\n",
    "\n",
    "# Evaluate option price\n",
    "option_price = evaluate_option_price(\n",
    "    spot_price_val, strike, expiry_val, rate_val, vol_val, num_steps_val, num_paths,\n",
    "    feature_functions, gamma, epsilon\n",
    ")\n",
    "\n",
    "print(f\"Estimated option price using LSPI: {option_price}\")\n",
    "\n",
    "\n",
    "# Define payoff function\n",
    "if is_call:\n",
    "    opt_payoff = lambda _, x: max(x - strike, 0)\n",
    "else:\n",
    "    opt_payoff = lambda _, x: max(strike - x, 0)\n",
    "\n",
    "# Instantiate and price using Binomial Tree\n",
    "bin_tree = OptimalExerciseBinTree(\n",
    "    spot_price=spot_price_val,\n",
    "    payoff=opt_payoff,\n",
    "    expiry=expiry_val,\n",
    "    rate=rate_val,\n",
    "    vol=vol_val,\n",
    "    num_steps=num_steps_val\n",
    ")\n",
    "_, policy_seq = zip(*bin_tree.get_opt_vf_and_policy())\n",
    "binomial_price = bin_tree.option_exercise_boundary(policy_seq, is_call)[0][1]\n",
    "print(f\"American Option Price using Binomial Tree = {binomial_price:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Callable, Iterable, TypeVar, Tuple\n",
    "from dataclasses import dataclass\n",
    "print(\"here\")\n",
    "# Assuming S and A are defined as before\n",
    "S = Tuple[float, float]  # State represented as (price, time)\n",
    "A = str  # Action represented as a string ('Hold' or 'Exercise')\n",
    "\n",
    "# Assuming TransitionStep is defined as before\n",
    "@dataclass(frozen=True)\n",
    "class TransitionStep:\n",
    "    state: Tuple[float, float]\n",
    "    action: str\n",
    "    next_state: Tuple[float, float]\n",
    "    reward: float\n",
    "\n",
    "# Define your neural network architecture here\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=100),  # Assuming state has 2 features\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=100, out_features=1)\n",
    ")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Implement Q-learning update rule\n",
    "def q_learning_update(model, transition: TransitionStep, gamma: float):\n",
    "    # Convert state and next_state to tensors\n",
    "    state_values = transition.state.state if isinstance(transition.state, NonTerminal) else transition.state\n",
    "    next_state_values = transition.next_state.state if isinstance(transition.next_state, NonTerminal) else (0, 0)  # Using (0, 0) as a dummy value for Terminal state\n",
    "\n",
    "    # Convert state and next_state to tensors\n",
    "    state_tensor = torch.tensor([state_values], dtype=torch.float32)\n",
    "    # Check if next_state is Terminal and handle accordingly\n",
    "    if isinstance(transition.next_state, Terminal):\n",
    "        next_state_tensor = None\n",
    "    else:\n",
    "        next_state_tensor = torch.tensor([next_state_values], dtype=torch.float32)\n",
    "\n",
    "    # Forward pass for the current state\n",
    "    q_value = model(state_tensor)\n",
    "\n",
    "    # Compute the target Q-value\n",
    "    with torch.no_grad():  # Ensure no gradients are computed in this block\n",
    "        if next_state_tensor is not None:\n",
    "            next_q_value = model(next_state_tensor)\n",
    "            max_next_q_value = torch.max(next_q_value)  # Modify if you have multiple actions\n",
    "            target_value = transition.reward + gamma * max_next_q_value\n",
    "        else:\n",
    "            # For terminal states, the target value is just the immediate reward\n",
    "            target_value = transition.reward\n",
    "\n",
    "    target_value_tensor = torch.tensor([[target_value]], dtype=torch.float32)  # Make sure it's the same shape as q_value\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = nn.functional.mse_loss(q_value, target_value_tensor)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Placeholder policy function\n",
    "def policy(state: Tuple[float, float], model: nn.Module) -> A:\n",
    "    # This function should use the model to choose the action\n",
    "    # Here we use a placeholder that always exercises\n",
    "    return 'Exercise'\n",
    "\n",
    "# Training loop\n",
    "def train_model(transitions: Iterable[TransitionStep], gamma: float, num_epochs: int):\n",
    "    for epoch in range(num_epochs):\n",
    "        for transition in transitions:\n",
    "            q_learning_update(model, transition, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_price = 100.0\n",
    "strike = 100.0\n",
    "expiry = 1.0\n",
    "rate = 0.05\n",
    "vol = 0.2\n",
    "num_steps = 10\n",
    "num_paths = 100\n",
    "gamma = np.exp(-rate_val * expiry_val / num_steps_val)  # Discount factor for one step\n",
    "epsilon = 0.01\n",
    "is_call = False  \n",
    "\n",
    "paths = simulate_asset_paths(spot_price, expiry, rate, vol, num_steps, num_paths)\n",
    "transitions = list(generate_transitions(paths, strike, rate, expiry, num_steps))\n",
    "train_model(transitions, gamma, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated American Option Price: 71.88808126441067\n"
     ]
    }
   ],
   "source": [
    "# Function to determine the action ('Hold' or 'Exercise') from the model\n",
    "def policy_decision(state: Tuple[float, float], model: torch.nn.Module) -> A:\n",
    "    state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "    with torch.no_grad():  # Do not track gradients during inference\n",
    "        q_value = model(state_tensor).item()\n",
    "    return 'Exercise' if q_value > 0 else 'Hold'  # Example decision rule\n",
    "\n",
    "# Function to estimate the option price\n",
    "def estimate_option_price(model, num_paths, rate, expiry, num_steps, strike, is_call):\n",
    "    paths = simulate_asset_paths(spot_price, expiry, rate, vol, num_steps, num_paths)\n",
    "    dt = expiry / num_steps\n",
    "    discount_factor = np.exp(-rate * dt)\n",
    "    total_payoff = 0.0\n",
    "\n",
    "    for path in paths:\n",
    "        payoff = 0.0\n",
    "        for t in range(num_steps):\n",
    "            current_price = path[t]\n",
    "            time_to_expiry = expiry - t * dt\n",
    "            state = (current_price, time_to_expiry)\n",
    "            action = policy_decision(state, model)\n",
    "\n",
    "            # if action == 'Exercise':\n",
    "            if t == num_steps - 1:\n",
    "                if is_call:\n",
    "                    payoff = max(current_price - strike, 0)\n",
    "                else:  # Put option\n",
    "                    payoff = max(strike - current_price, 0)\n",
    "                total_payoff += payoff * discount_factor ** t\n",
    "                break  # Stop evaluating this path further if exercised\n",
    "\n",
    "    return total_payoff * num_steps / num_paths  # Average payoff to estimate option price\n",
    "\n",
    "# Evaluate the option price using the trained model\n",
    "estimated_option_price = estimate_option_price(model, num_paths, rate, expiry, num_steps, strike, is_call)\n",
    "print(f\"Estimated American Option Price: {estimated_option_price}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "752579dbebe7f4dfe7c1aa72eac13e23fc88be2cc1ea7ab14e1f8d69b2d97d12"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
